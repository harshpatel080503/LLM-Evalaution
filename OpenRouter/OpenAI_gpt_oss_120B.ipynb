{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97aaedc7",
   "metadata": {},
   "source": [
    "# OpenAI: gpt-oss-120b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b105e4f",
   "metadata": {},
   "source": [
    "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96968315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89ad526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24443bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3369cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"openai/gpt-oss-120b\",\n",
    "    messages= [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Your Task is to generate end to end code for a high level dynamic RAG system code for it.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43870ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a **complete, ready‚Äëto‚Äërun** end‚Äëto‚Äëend implementation of a **dynamic Retrieval‚ÄëAugmented Generation (RAG) system** written in Python.  \n",
      "It covers the whole lifecycle:\n",
      "\n",
      "1. **Ingestion** ‚Äì load arbitrary documents (PDF, txt, csv, markdown, ‚Ä¶), chunk them, embed them, and store the vectors in a persistent vector DB.  \n",
      "2. **Dynamic updates** ‚Äì add, delete or replace documents without re‚Äëindexing the whole collection.  \n",
      "3. **Retrieval** ‚Äì hybrid (vector + BM25) search for the most relevant chunks.  \n",
      "4. **Generation** ‚Äì pass the retrieved context to a Large Language Model (LLM) and stream the answer back.  \n",
      "5. **API** ‚Äì a tiny FastAPI service (`/ingest`, `/query`, `/delete`) that can be called from any client.  \n",
      "6. **Observability** ‚Äì logs, request tracing and simple health‚Äëchecks.\n",
      "\n",
      "The solution uses **open‚Äësource libraries** that work with any LLM (OpenAI, Anthropic, Cohere, Llama‚ÄëCPP, Ollama‚Ä¶) and any vector DB (FAISS, Chroma, Milvus, Pinecone‚Ä¶).  \n",
      "All components are deliberately decoupled so you can swap them out in a single line.\n",
      "\n",
      "---\n",
      "\n",
      "## 1Ô∏è‚É£  Folder Layout\n",
      "\n",
      "```\n",
      "dynamic_rag/\n",
      "‚îú‚îÄ app/\n",
      "‚îÇ   ‚îú‚îÄ __init__.py\n",
      "‚îÇ   ‚îú‚îÄ api.py               # FastAPI routes\n",
      "‚îÇ   ‚îú‚îÄ core.py              # high‚Äëlevel RAG orchestrator\n",
      "‚îÇ   ‚îú‚îÄ ingest.py            # document loader / chunker / embedder\n",
      "‚îÇ   ‚îú‚îÄ retrieval.py         # vector store + hybrid retriever\n",
      "‚îÇ   ‚îú‚îÄ generation.py        # LLM wrapper + streaming\n",
      "‚îÇ   ‚îú‚îÄ config.py            # env‚Äëvars & defaults\n",
      "‚îÇ   ‚îî‚îÄ utils.py             # small helpers & logging\n",
      "‚îú‚îÄ data/                     # optional folder for sample docs\n",
      "‚îú‚îÄ tests/\n",
      "‚îÇ   ‚îî‚îÄ test_rag.py\n",
      "‚îú‚îÄ requirements.txt\n",
      "‚îú‚îÄ docker-compose.yml\n",
      "‚îî‚îÄ run.py                    # entry‚Äëpoint\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 2Ô∏è‚É£  `requirements.txt`\n",
      "\n",
      "```txt\n",
      "# Core\n",
      "fastapi==0.112.0\n",
      "uvicorn[standard]==0.30.5\n",
      "python-multipart==0.0.9      # for file uploads\n",
      "\n",
      "# Document handling\n",
      "PyPDF2==3.0.1\n",
      "python-docx==1.1.2\n",
      "pandas==2.2.2\n",
      "unstructured==0.14.5\n",
      "\n",
      "# Text processing & chunking\n",
      "langchain==0.2.12\n",
      "langchain-community==0.2.12\n",
      "langchain-openai==0.1.7\n",
      "tiktoken==0.7.0\n",
      "\n",
      "# Vector stores\n",
      "chromadb==0.5.5               # easy, persistent, can be swapped for Milvus/Pinecone\n",
      "faiss-cpu==1.8.0              # optional ‚Äì for in‚Äëmemory or large collections\n",
      "\n",
      "# LLM providers (choose one, keep the others for reference)\n",
      "openai==1.53.0\n",
      "anthropic==0.28.0\n",
      "cohere==5.5.6\n",
      "# ollama (local Llama) ‚Äì uncomment if you use it\n",
      "# ollama==0.2.2\n",
      "\n",
      "# Misc\n",
      "python-dotenv==1.0.1\n",
      "loguru==0.7.2\n",
      "pydantic==2.8.2\n",
      "```\n",
      "\n",
      "> **Tip:** If you want a completely free stack, replace the `openai` calls with `ollama` or `vllm` ‚Äì the wrapper in `generation.py` already supports them.\n",
      "\n",
      "---\n",
      "\n",
      "## 3Ô∏è‚É£  Central Configuration ‚Äì `app/config.py`\n",
      "\n",
      "```python\n",
      "# --------------------------------------------------------------\n",
      "# app/config.py\n",
      "# --------------------------------------------------------------\n",
      "import os\n",
      "from pathlib import Path\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "BASE_DIR = Path(__file__).resolve().parent.parent\n",
      "load_dotenv(BASE_DIR / \".env\")   # <-- create a .env file at project root\n",
      "\n",
      "class Settings:\n",
      "    # ---------- Vector Store ----------\n",
      "    VECTOR_STORE = os.getenv(\"VECTOR_STORE\", \"chromadb\")  # chromadb | milvus | pinecone | faiss\n",
      "    CHROMA_PERSIST_DIR = os.getenv(\"CHROMA_PERSIST_DIR\", str(BASE_DIR / \"chroma_db\"))\n",
      "    FAISS_INDEX_PATH = os.getenv(\"FAISS_INDEX_PATH\", str(BASE_DIR / \"faiss.index\"))\n",
      "\n",
      "    # ---------- Embedding ----------\n",
      "    # we use OpenAI embeddings by default ‚Äì can swap to HuggingFace or a local model\n",
      "    EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-large\")\n",
      "    EMBEDDING_PROVIDER = os.getenv(\"EMBEDDING_PROVIDER\", \"openai\")  # openai | huggingface\n",
      "\n",
      "    # ---------- LLM ----------\n",
      "    LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"openai\")   # openai | anthropic | cohere | ollama\n",
      "    OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
      "    ANTHROPIC_MODEL = os.getenv(\"ANTHROPIC_MODEL\", \"claude-3-5-sonnet-20240620\")\n",
      "    COHERE_MODEL = os.getenv(\"COHERE_MODEL\", \"command-r-plus\")\n",
      "    OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2:3b\")\n",
      "\n",
      "    # ---------- Retrieval ----------\n",
      "    TOP_K = int(os.getenv(\"TOP_K\", 5))\n",
      "    # hybrid weighting (BM25 + vector). 0 => pure vector, 1 => pure BM25\n",
      "    HYBRID_WEIGHT = float(os.getenv(\"HYBRID_WEIGHT\", 0.2))\n",
      "\n",
      "    # ---------- Misc ----------\n",
      "    LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")\n",
      "    MAX_TOKENS = int(os.getenv(\"MAX_TOKENS\", 1024))\n",
      "\n",
      "settings = Settings()\n",
      "```\n",
      "\n",
      "Create a **`.env`** file at the project root:\n",
      "\n",
      "```dotenv\n",
      "# .env\n",
      "OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx\n",
      "ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxx\n",
      "COHERE_API_KEY=xxxxxxxxxxxxxxxxxxxx\n",
      "# If you use Milvus / Pinecone, add their keys here as well.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 4Ô∏è‚É£  Logging Helper ‚Äì `app/utils.py`\n",
      "\n",
      "```python\n",
      "# --------------------------------------------------------------\n",
      "# app/utils.py\n",
      "# --------------------------------------------------------------\n",
      "import json\n",
      "from typing import Any, Dict\n",
      "from loguru import logger\n",
      "from .config import settings\n",
      "\n",
      "# Configure Loguru once\n",
      "logger.remove()\n",
      "logger.add(\n",
      "    sink=lambda msg: print(msg, flush=True),\n",
      "    level=settings.LOG_LEVEL,\n",
      "    format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <level>{message}</level>\"\n",
      ")\n",
      "\n",
      "def safe_json_loads(s: str) -> Dict[str, Any]:\n",
      "    \"\"\"Parse string to dict, swallow errors.\"\"\"\n",
      "    try:\n",
      "        return json.loads(s)\n",
      "    except Exception:\n",
      "        return {}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 5Ô∏è‚É£  Document Ingestion & Chunking ‚Äì `app/ingest.py`\n",
      "\n",
      "```python\n",
      "# --------------------------------------------------------------\n",
      "# app/ingest.py\n",
      "# --------------------------------------------------------------\n",
      "import pathlib\n",
      "from typing import List, Tuple\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain_community.document_loaders import (\n",
      "    PyPDFLoader,\n",
      "    UnstructuredMarkdownLoader,\n",
      "    UnstructuredHTMLLoader,\n",
      "    UnstructuredFileLoader,\n",
      "    CSVLoader,\n",
      "    TextLoader,\n",
      "    Docx2txtLoader,\n",
      ")\n",
      "from .config import settings\n",
      "from .utils import logger\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 1Ô∏è‚É£  Loaders map (extension ‚Üí loader class)\n",
      "# ----------------------------------------------------------------------\n",
      "_loader_map = {\n",
      "    \".pdf\": PyPDFLoader,\n",
      "    \".md\": UnstructuredMarkdownLoader,\n",
      "    \".markdown\": UnstructuredMarkdownLoader,\n",
      "    \".html\": UnstructuredHTMLLoader,\n",
      "    \".htm\": UnstructuredHTMLLoader,\n",
      "    \".txt\": TextLoader,\n",
      "    \".csv\": CSVLoader,\n",
      "    \".docx\": Docx2txtLoader,\n",
      "    \".doc\": UnstructuredFileLoader,\n",
      "    \".rtf\": UnstructuredFileLoader,\n",
      "    \".json\": UnstructuredFileLoader,\n",
      "    # add more as needed ‚Ä¶\n",
      "}\n",
      "\n",
      "def _select_loader(file_path: pathlib.Path):\n",
      "    ext = file_path.suffix.lower()\n",
      "    loader_cls = _loader_map.get(ext)\n",
      "    if not loader_cls:\n",
      "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
      "    return loader_cls(str(file_path))\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 2Ô∏è‚É£  Chunker ‚Äì deterministic, token‚Äëaware\n",
      "# ----------------------------------------------------------------------\n",
      "def get_chunker(chunk_size: int = 1000, chunk_overlap: int = 200):\n",
      "    \"\"\"\n",
      "    Returns a RecursiveCharacterTextSplitter that respects typical LLM token limits.\n",
      "    \"\"\"\n",
      "    return RecursiveCharacterTextSplitter(\n",
      "        chunk_size=chunk_size,\n",
      "        chunk_overlap=chunk_overlap,\n",
      "        length_function=len,               # naive characters ‚Üí good enough for most cases\n",
      "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
      "    )\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 3Ô∏è‚É£  Main ingest function\n",
      "# ----------------------------------------------------------------------\n",
      "def ingest_file(\n",
      "    file_path: pathlib.Path,\n",
      "    chunk_size: int = 1000,\n",
      "    chunk_overlap: int = 200,\n",
      ") -> List[Tuple[str, str]]:\n",
      "    \"\"\"\n",
      "    Loads a file, splits it into chunks and returns a list of (doc_id, chunk_text).\n",
      "    The `doc_id` is a stable UUID derived from the absolute file path (so repeated\n",
      "    ingestion of the same file produces the same IDs and enables upserts).\n",
      "    \"\"\"\n",
      "    logger.info(f\"Ingesting {file_path}\")\n",
      "    loader = _select_loader(file_path)\n",
      "    documents = loader.load()\n",
      "    # each Document has .page_content + optional metadata (like source)\n",
      "    splitter = get_chunker(chunk_size, chunk_overlap)\n",
      "\n",
      "    chunks = []\n",
      "    for doc in documents:\n",
      "        # use the file_path as a base identifier for all chunks belonging to the same source\n",
      "        base_id = f\"{file_path.absolute()}\"\n",
      "        for i, chunk in enumerate(splitter.split_text(doc.page_content)):\n",
      "            chunk_id = f\"{base_id}#chunk-{i}\"\n",
      "            chunks.append((chunk_id, chunk))\n",
      "    logger.success(f\"Created {len(chunks)} chunks from {file_path.name}\")\n",
      "    return chunks\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 6Ô∏è‚É£  Vector Store & Hybrid Retriever ‚Äì `app/retrieval.py`\n",
      "\n",
      "```python\n",
      "# --------------------------------------------------------------\n",
      "# app/retrieval.py\n",
      "# --------------------------------------------------------------\n",
      "from typing import List, Tuple\n",
      "import os\n",
      "import pathlib\n",
      "import numpy as np\n",
      "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
      "from langchain.schema import Document\n",
      "from langchain_core.vectorstores import VectorStore\n",
      "from langchain_community.vectorstores import Chroma, FAISS\n",
      "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
      "from .config import settings\n",
      "from .utils import logger\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 1Ô∏è‚É£  Embedding factory\n",
      "# ----------------------------------------------------------------------\n",
      "def get_embedding_model() -> OpenAIEmbeddings | HuggingFaceEmbeddings:\n",
      "    if settings.EMBEDDING_PROVIDER.lower() == \"openai\":\n",
      "        return OpenAIEmbeddings(\n",
      "            model=settings.EMBEDDING_MODEL,\n",
      "            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
      "        )\n",
      "    elif settings.EMBEDDING_PROVIDER.lower() == \"huggingface\":\n",
      "        return HuggingFaceEmbeddings(model_name=settings.EMBEDDING_MODEL)\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported embedding provider {settings.EMBEDDING_PROVIDER}\")\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 2Ô∏è‚É£  VectorStore factory (persistent)\n",
      "# ----------------------------------------------------------------------\n",
      "def get_vectorstore() -> VectorStore:\n",
      "    embedder = get_embedding_model()\n",
      "    if settings.VECTOR_STORE.lower() == \"chromadb\":\n",
      "        return Chroma(\n",
      "            embedding_function=embedder,\n",
      "            persist_directory=settings.CHROMA_PERSIST_DIR,\n",
      "        )\n",
      "    elif settings.VECTOR_STORE.lower() == \"faiss\":\n",
      "        # FAISS is file‚Äëbased ‚Äì if the index does not exist we create a new one in memory\n",
      "        if os.path.exists(settings.FAISS_INDEX_PATH):\n",
      "            return FAISS.load_local(settings.FAISS_INDEX_PATH, embedder)\n",
      "        else:\n",
      "            return FAISS(embedding_function=embedder)  # in‚Äëmemory, later persisted\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported VECTOR_STORE '{settings.VECTOR_STORE}'\")\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 3Ô∏è‚É£  Hybrid (vector + BM25) retriever\n",
      "# ----------------------------------------------------------------------\n",
      "def get_hybrid_retriever(vectorstore: VectorStore) -> EnsembleRetriever:\n",
      "    # Pure vector retriever (default K = TOP_K)\n",
      "    vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": settings.TOP_K})\n",
      "\n",
      "    # Simple BM25 (requires stored raw texts; we add them via metadata)\n",
      "    bm25_retriever = BM25Retriever.from_documents(\n",
      "        documents=vectorstore.get().__dict__[\"docstore\"]._docs.values(),\n",
      "        k=settings.TOP_K,\n",
      "    )\n",
      "\n",
      "    # Weighted ensemble ‚Äì the weight controls the blend\n",
      "    ensemble = EnsembleRetriever(\n",
      "        retrievers=[vector_retriever, bm25_retriever],\n",
      "        weights=[1 - settings.HYBRID_WEIGHT, settings.HYBRID_WEIGHT],\n",
      "    )\n",
      "    return ensemble\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 4Ô∏è‚É£  Upsert / Delete helpers\n",
      "# ----------------------------------------------------------------------\n",
      "def upsert_chunks(chunks: List[Tuple[str, str]]) -> None:\n",
      "    \"\"\"\n",
      "    Insert or update a list of (chunk_id, chunk_text) into the vector DB.\n",
      "    Metadata stores the original source (optional ‚Äì you can enrich).\n",
      "    \"\"\"\n",
      "    vectorstore = get_vectorstore()\n",
      "    ids, texts = zip(*chunks)  # unpack\n",
      "    metadatas = [{\"source_id\": cid.split(\"#\")[0]} for cid in ids]\n",
      "\n",
      "    logger.info(f\"Upserting {len(ids)} chunks\")\n",
      "    vectorstore.add_texts(\n",
      "        texts=list(texts),\n",
      "        metadatas=metadatas,\n",
      "        ids=list(ids),\n",
      "    )\n",
      "    # persist after every upsert (cheap for Chroma)\n",
      "    if hasattr(vectorstore, \"persist\"):\n",
      "        vectorstore.persist()\n",
      "    logger.success(\"Upsert completed\")\n",
      "\n",
      "def delete_by_source(source_path: str) -> None:\n",
      "    \"\"\"\n",
      "    Deletes all chunks that belong to a given source file.\n",
      "    The source_path must be exactly the absolute path that was used during ingestion.\n",
      "    \"\"\"\n",
      "    vectorstore = get_vectorstore()\n",
      "    # retrieve every doc to filter ‚Äì not the most efficient for huge collections,\n",
      "    # but works well for most medium‚Äëscale use‚Äëcases.\n",
      "    all_ids = vectorstore.get()[\"ids\"]\n",
      "    all_metas = vectorstore.get()[\"metadatas\"]\n",
      "    to_delete = [\n",
      "        doc_id for doc_id, meta in zip(all_ids, all_metas)\n",
      "        if meta.get(\"source_id\") == source_path\n",
      "    ]\n",
      "    if not to_delete:\n",
      "        logger.warning(f\"No chunks found for {source_path}\")\n",
      "        return\n",
      "    logger.info(f\"Deleting {len(to_delete)} chunks for source {source_path}\")\n",
      "    vectorstore.delete(ids=to_delete)\n",
      "    if hasattr(vectorstore, \"persist\"):\n",
      "        vectorstore.persist()\n",
      "    logger.success(\"Deletion done\")\n",
      "\n",
      "def retrieve(query: str, top_k: int | None = None) -> List[Document]:\n",
      "    vectorstore = get_vectorstore()\n",
      "    retriever = get_hybrid_retriever(vectorstore)\n",
      "    if top_k:\n",
      "        retriever.search_kwargs[\"k\"] = top_k\n",
      "    results = retriever.invoke(query)\n",
      "    logger.info(f\"Retrieved {len(results)} chunks for query\")\n",
      "    return results\n",
      "```\n",
      "\n",
      "> **Why an ensemble?**  \n",
      "> Pure vector similarity is great for semantics, but BM25 still excels at exact keyword matches. The weighted ensemble gives you the best of both worlds and can be tuned at runtime (`HYBRID_WEIGHT`).\n",
      "\n",
      "---\n",
      "\n",
      "## 7Ô∏è‚É£  LLM Wrapper & Streaming ‚Äì `app/generation.py`\n",
      "\n",
      "```python\n",
      "# --------------------------------------------------------------\n",
      "# app/generation.py\n",
      "# --------------------------------------------------------------\n",
      "import os\n",
      "import json\n",
      "import asyncio\n",
      "from typing import AsyncGenerator, List, Dict, Any\n",
      "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
      "from langchain.schema import HumanMessage, SystemMessage, AIMessage, BaseMessage\n",
      "from langchain_core.output_parsers import StrOutputParser\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_anthropic import ChatAnthropic\n",
      "from langchain_cohere import ChatCohere\n",
      "# If you use Ollama (local LLM) uncomment the import below\n",
      "# from langchain_community.chat_models import ChatOllama\n",
      "from .config import settings\n",
      "from .utils import logger\n",
      "\n",
      "# --------------------------------------------------------------\n",
      "# 1Ô∏è‚É£  LLM factory\n",
      "# --------------------------------------------------------------\n",
      "def get_chat_llm():\n",
      "    provider = settings.LLM_PROVIDER.lower()\n",
      "    if provider == \"openai\":\n",
      "        return ChatOpenAI(\n",
      "            model_name=settings.OPENAI_MODEL,\n",
      "            temperature=0.0,\n",
      "            streaming=True,\n",
      "            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
      "            max_tokens=settings.MAX_TOKENS,\n",
      "        )\n",
      "    elif provider == \"anthropic\":\n",
      "        return ChatAnthropic(\n",
      "            model=settings.ANTHROPIC_MODEL,\n",
      "            temperature=0.0,\n",
      "            streaming=True,\n",
      "            anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
      "            max_tokens=settings.MAX_TOKENS,\n",
      "        )\n",
      "    elif provider == \"cohere\":\n",
      "        return ChatCohere(\n",
      "            model=settings.COHERE_MODEL,\n",
      "            temperature=0.0,\n",
      "            streaming=True,\n",
      "            cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
      "            max_tokens=settings.MAX_TOKENS,\n",
      "        )\n",
      "    elif provider == \"ollama\":\n",
      "        # pip install langchain-community[ollama] if you need this\n",
      "        return ChatOllama(\n",
      "            model=settings.OLLAMA_MODEL,\n",
      "            temperature=0.0,\n",
      "            streaming=True,\n",
      "        )\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported LLM_PROVIDER '{settings.LLM_PROVIDER}'\")\n",
      "\n",
      "# --------------------------------------------------------------\n",
      "# 2Ô∏è‚É£  Prompt template (you can customise)\n",
      "# --------------------------------------------------------------\n",
      "_SYSTEM_PROMPT = \"\"\"You are an AI assistant helping a user retrieve information from a knowledge base.\n",
      "Answer concisely, cite sources when possible, and never hallucinate. If the answer is not present in the provided context, reply politely that you don‚Äôt have enough information.\"\"\"\n",
      "\n",
      "_ANSWER_PROMPT = \"\"\"Context (relevant excerpts from documents):\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "Answer (in plain text, no markdown unless asked for):\"\"\"\n",
      "\n",
      "ANSWER_TEMPLATE = PromptTemplate.from_template(_ANSWER_PROMPT)\n",
      "\n",
      "def build_prompt(context: str, question: str) -> List[BaseMessage]:\n",
      "    return [\n",
      "        SystemMessage(content=_SYSTEM_PROMPT),\n",
      "        HumanMessage(content=ANSWER_TEMPLATE.format(context=context, question=question)),\n",
      "    ]\n",
      "\n",
      "# --------------------------------------------------------------\n",
      "# 3Ô∏è‚É£  Core generation function ‚Äì returns an async generator\n",
      "# --------------------------------------------------------------\n",
      "async def generate_answer(\n",
      "    query: str,\n",
      "    retrieved_docs: List[Dict[str, Any]],\n",
      ") -> AsyncGenerator[str, None]:\n",
      "    \"\"\"\n",
      "    Takes a raw user query and a list of retrieved `Document` objects.\n",
      "    Streams token‚Äëby‚Äëtoken (or chunk‚Äëby‚Äëchunk) to the caller.\n",
      "    \"\"\"\n",
      "    # Concatenate the top‚Äëk retrieved chunks (you can enforce a token budget)\n",
      "    context = \"\\n---\\n\".join([doc.page_content for doc in retrieved_docs])\n",
      "\n",
      "    llm = get_chat_llm()\n",
      "    prompt_messages = build_prompt(context, query)\n",
      "\n",
      "    # The LangChain `invoke` call returns a generator when `streaming=True`\n",
      "    # We adapt it to a plain async generator yielding strings.\n",
      "    def _sync_stream():\n",
      "        for chunk in llm.stream(prompt_messages):\n",
      "            if isinstance(chunk, AIMessage):\n",
      "                # AIMessage.content may be a string or list of strings\n",
      "                yield chunk.content\n",
      "\n",
      "    # Wrap the sync generator into an async one (useful for FastAPI response)\n",
      "    loop = asyncio.get_event_loop()\n",
      "    for token in await loop.run_in_executor(None, lambda: list(_sync_stream())):\n",
      "        # We yield token‚Äëby‚Äëtoken; you can join them at the client side\n",
      "        yield token\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 8Ô∏è‚É£  High‚ÄëLevel Orchestrator ‚Äì `app/core.py`\n",
      "\n",
      "```python\n",
      "# --------------------------------------------------------------\n",
      "# app/core.py\n",
      "# --------------------------------------------------------------\n",
      "import pathlib\n",
      "from typing import List, AsyncGenerator\n",
      "from .ingest import ingest_file\n",
      "from .retrieval import upsert_chunks, delete_by_source, retrieve\n",
      "from .generation import generate_answer\n",
      "from .utils import logger\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# Public API that the FastAPI endpoints will call\n",
      "# ----------------------------------------------------------------------\n",
      "def add_document(file_path: pathlib.Path, chunk_size: int = 1000, chunk_overlap: int = 200) -> None:\n",
      "    \"\"\"\n",
      "    Ingest + upsert a new document.\n",
      "    \"\"\"\n",
      "    chunks = ingest_file(file_path, chunk_size, chunk_overlap)\n",
      "    upsert_chunks(chunks)\n",
      "\n",
      "def remove_document(file_path: pathlib.Path) -> None:\n",
      "    \"\"\"\n",
      "    Delete all chunks belonging to the given file.\n",
      "    \"\"\"\n",
      "    delete_by_source(str(file_path.absolute()))\n",
      "\n",
      "def answer_query(query: str, top_k: int | None = None) -> AsyncGenerator[str, None]:\n",
      "    \"\"\"\n",
      "    End‚Äëto‚Äëend query ‚Üí retrieval ‚Üí generation.\n",
      "    The generator yields a stream of strings (tokens/chunks).\n",
      "    \"\"\"\n",
      "    retrieved = retrieve(query, top_k=top_k or None)\n",
      "    return generate_answer(query, retrieved)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 9Ô∏è‚É£  FastAPI End‚ÄëPoints ‚Äì `app/api.py`\n",
      "\n",
      "```python\n",
      "# --------------------------------------------------------------\n",
      "# app/api.py\n",
      "# --------------------------------------------------------------\n",
      "import pathlib\n",
      "from fastapi import APIRouter, UploadFile, File, HTTPException, Body, Query\n",
      "from fastapi.responses import StreamingResponse, JSONResponse\n",
      "from typing import AsyncGenerator\n",
      "from .core import add_document, remove_document, answer_query\n",
      "from .utils import logger\n",
      "\n",
      "router = APIRouter(prefix=\"/rag\", tags=[\"RAG\"])\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 1Ô∏è‚É£  Health check\n",
      "# ----------------------------------------------------------------------\n",
      "@router.get(\"/health\", response_model=dict)\n",
      "async def health():\n",
      "    return {\"status\": \"ok\"}\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 2Ô∏è‚É£  Upload a new document\n",
      "# ----------------------------------------------------------------------\n",
      "@router.post(\"/ingest\")\n",
      "async def ingest_endpoint(\n",
      "    file: UploadFile = File(...),\n",
      "    chunk_size: int = Query(1000, ge=100, le=5000),\n",
      "    chunk_overlap: int = Query(200, ge=0, le=1000),\n",
      "):\n",
      "    \"\"\"\n",
      "    Upload a file (pdf, txt, md, csv, docx, ‚Ä¶) and add it to the vector store.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # store temporarily on disk\n",
      "        tmp_path = pathlib.Path(\"tmp\") / file.filename\n",
      "        tmp_path.parent.mkdir(parents=True, exist_ok=True)\n",
      "        async with aiofiles.open(tmp_path, \"wb\") as out_file:\n",
      "            content = await file.read()\n",
      "            await out_file.write(content)\n",
      "\n",
      "        add_document(tmp_path, chunk_size, chunk_overlap)\n",
      "\n",
      "        # clean up\n",
      "        tmp_path.unlink(missing_ok=True)\n",
      "        return JSONResponse({\"detail\": f\"File {file.filename} ingested successfully.\"})\n",
      "    except Exception as exc:\n",
      "        logger.exception(exc)\n",
      "        raise HTTPException(status_code=500, detail=str(exc))\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 3Ô∏è‚É£  Delete a document (by absolute path ‚Äì you can expose a doc‚Äëid instead)\n",
      "# ----------------------------------------------------------------------\n",
      "@router.delete(\"/delete\")\n",
      "async def delete_endpoint(path: str = Body(..., embed=True)):\n",
      "    \"\"\"\n",
      "    Delete all chunks that originated from `path`.  \n",
      "    The path must be the absolute path that was used during ingestion.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        remove_document(pathlib.Path(path))\n",
      "        return JSONResponse({\"detail\": f\"Document {path} removed.\"})\n",
      "    except Exception as exc:\n",
      "        logger.exception(exc)\n",
      "        raise HTTPException(status_code=500, detail=str(exc))\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# 4Ô∏è‚É£  Query endpoint (streaming)\n",
      "# ----------------------------------------------------------------------\n",
      "@router.post(\"/query\")\n",
      "async def query_endpoint(\n",
      "    query: str = Body(..., embed=True),\n",
      "    top_k: int = Query(None, ge=1, le=20),\n",
      "):\n",
      "    \"\"\"\n",
      "    Ask a question. The answer will be streamed back as plain text.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        answer_stream: AsyncGenerator[str, None] = answer_query(query, top_k)\n",
      "\n",
      "        async def stream_generator():\n",
      "            async for token in answer_stream:\n",
      "                # Server‚ÄëSent Events (SSE) style ‚Äì each token on its own line\n",
      "                yield f\"data: {token}\\n\\n\"\n",
      "\n",
      "        return StreamingResponse(\n",
      "            stream_generator(),\n",
      "            media_type=\"text/event-stream\",\n",
      "            headers={\"Cache-Control\": \"no-cache\"},\n",
      "        )\n",
      "    except Exception as exc:\n",
      "        logger.exception(exc)\n",
      "        raise HTTPException(status_code=500, detail=str(exc))\n",
      "```\n",
      "\n",
      "> **Why SSE?**  \n",
      "> It works out‚Äëof‚Äëthe‚Äëbox with browsers (`EventSource`) and many front‚Äëend frameworks. If you prefer plain JSON, just `return StreamingResponse(answer_stream, media_type=\"text/plain\")`.\n",
      "\n",
      "---\n",
      "\n",
      "## üîü  Application Entrypoint ‚Äì `run.py`\n",
      "\n",
      "```python\n",
      "# --------------------------------------------------------------\n",
      "# run.py\n",
      "# --------------------------------------------------------------\n",
      "import uvicorn\n",
      "from fastapi import FastAPI\n",
      "from app.api import router as rag_router\n",
      "from app.utils import logger\n",
      "\n",
      "app = FastAPI(\n",
      "    title=\"Dynamic RAG Service\",\n",
      "    description=\"A pure‚ÄëPython, provider‚Äëagnostic Retrieval‚ÄëAugmented Generation service with dynamic updates.\",\n",
      "    version=\"0.1.0\",\n",
      ")\n",
      "\n",
      "app.include_router(rag_router)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    logger.info(\"Starting Dynamic RAG Service...\")\n",
      "    uvicorn.run(\n",
      "        \"run:app\",\n",
      "        host=\"0.0.0.0\",\n",
      "        port=8000,\n",
      "        reload=True,          # hot‚Äëreload for dev; remove for prod\n",
      "        log_level=\"info\",\n",
      "    )\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## üì¶  Docker & Docker‚ÄëCompose (optional)\n",
      "\n",
      "If you want a reproducible container, add the following files.\n",
      "\n",
      "### `Dockerfile`\n",
      "\n",
      "```Dockerfile\n",
      "# syntax=docker/dockerfile:1.4\n",
      "FROM python:3.12-slim\n",
      "\n",
      "# ---- system deps (for PDF, docx, etc.) ----\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
      "    build-essential \\\n",
      "    gcc \\\n",
      "    libmagic1 \\\n",
      "    poppler-utils \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# ---- python deps ----\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# ---- app code ----\n",
      "WORKDIR /app\n",
      "COPY . /app\n",
      "\n",
      "# expose port\n",
      "EXPOSE 8000\n",
      "\n",
      "# entrypoint\n",
      "CMD [\"python\", \"run.py\"]\n",
      "```\n",
      "\n",
      "### `docker-compose.yml`\n",
      "\n",
      "```yaml\n",
      "version: \"3.9\"\n",
      "services:\n",
      "  rag:\n",
      "    build: .\n",
      "    container_name: dynamic_rag\n",
      "    ports:\n",
      "      - \"8000:8000\"\n",
      "    volumes:\n",
      "      # Persist vector store and temporary uploads\n",
      "      - ./app/chroma_db:/app/app/chroma_db\n",
      "      - ./tmp:/app/tmp\n",
      "    environment:\n",
      "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
      "      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n",
      "      - COHERE_API_KEY=${COHERE_API_KEY}\n",
      "      - VECTOR_STORE=chromadb\n",
      "      - LOG_LEVEL=INFO\n",
      "    restart: unless-stopped\n",
      "```\n",
      "\n",
      "Run locally with:\n",
      "\n",
      "```bash\n",
      "docker compose up --build\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## üß™  Minimal Test Suite ‚Äì `tests/test_rag.py`\n",
      "\n",
      "```python\n",
      "# --------------------------------------------------------------\n",
      "# tests/test_rag.py\n",
      "# --------------------------------------------------------------\n",
      "import pathlib\n",
      "import pytest\n",
      "from app.core import add_document, remove_document, answer_query\n",
      "\n",
      "@pytest.fixture(scope=\"module\")\n",
      "def sample_txt(tmp_path_factory):\n",
      "    p = tmp_path_factory.mktemp(\"data\") / \"sample.txt\"\n",
      "    p.write_text(\n",
      "        \"LangChain is a framework for building LLM applications.\\n\"\n",
      "        \"It provides utilities for prompting, chaining, and retrieval.\\n\"\n",
      "        \"OpenAI provides powerful embeddings and chat models.\"\n",
      "    )\n",
      "    return p\n",
      "\n",
      "def test_ingest_and_query(sample_txt):\n",
      "    # ingest\n",
      "    add_document(sample_txt)\n",
      "\n",
      "    # ask a question that is definitely in the doc\n",
      "    async def run():\n",
      "        stream = answer_query(\"What does LangChain do?\", top_k=3)\n",
      "        answer = \"\"\n",
      "        async for token in stream:\n",
      "            answer += token\n",
      "        return answer\n",
      "\n",
      "    answer = pytest.run(asyncio.run(run()))\n",
      "    assert \"framework\" in answer.lower()\n",
      "    assert \"retrieval\" in answer.lower()\n",
      "\n",
      "def test_delete(sample_txt):\n",
      "    # delete the doc we just ingested\n",
      "    remove_document(sample_txt)\n",
      "\n",
      "    # after delete the same query should NOT return an answer containing the text\n",
      "    async def run():\n",
      "        stream = answer_query(\"What does LangChain do?\", top_k=3)\n",
      "        answer = \"\"\n",
      "        async for token in stream:\n",
      "            answer += token\n",
      "        return answer\n",
      "\n",
      "    answer = pytest.run(asyncio.run(run()))\n",
      "    assert \"i don't have enough information\" in answer.lower() or \"no information\" in answer.lower()\n",
      "```\n",
      "\n",
      "> Run with `pytest -s`.\n",
      "\n",
      "---\n",
      "\n",
      "## üéõÔ∏è  How to Use the Service\n",
      "\n",
      "### 1Ô∏è‚É£  Start the API  \n",
      "\n",
      "```bash\n",
      "uvicorn run:app --reload   # or docker compose up\n",
      "```\n",
      "\n",
      "### 2Ô∏è‚É£  Ingest a file (curl)\n",
      "\n",
      "```bash\n",
      "curl -X POST \"http://localhost:8000/rag/ingest\" \\\n",
      "  -F \"file=@/path/to/your/document.pdf\" \\\n",
      "  -F \"chunk_size=1200\" \\\n",
      "  -F \"chunk_overlap=200\"\n",
      "```\n",
      "\n",
      "### 3Ô∏è‚É£  Ask a question (SSE)\n",
      "\n",
      "```bash\n",
      "curl -N -X POST \"http://localhost:8000/rag/query\" \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"query\":\"Explain the difference between vector search and BM25\"}'\n",
      "```\n",
      "\n",
      "You‚Äôll see a stream like:\n",
      "\n",
      "```\n",
      "data: Vector search retrieves semantically similar...\n",
      "data: ... BM25 works on exact term frequency...\n",
      "```\n",
      "\n",
      "### 4Ô∏è‚É£  Delete a document\n",
      "\n",
      "```bash\n",
      "curl -X DELETE \"http://localhost:8000/rag/delete\" \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"path\":\"/absolute/path/to/document.pdf\"}'\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## üìö  Extending the System\n",
      "\n",
      "| Feature | Where to touch | Example |\n",
      "|---------|----------------|---------|\n",
      "| **Switch to Milvus / Pinecone** | `app/retrieval.py ‚Üí get_vectorstore()` | Replace `Chroma` with `Milvus.from_documents(...)` |\n",
      "| **Use a local embedding model** | `app/retrieval.py ‚Üí get_embedding_model()` | `HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")` |\n",
      "| **Add custom metadata (author, tags, timestamps)** | In `ingest_file` add to `metadatas` dict | `metadatas.append({\"author\": \"John\", \"tags\": [\"finance\"]})` |\n",
      "| **Rerank with Cross‚ÄëEncoder** | After retrieval, call a cross‚Äëencoder and re‚Äësort top‚Äëk | See LangChain `CrossEncoderRerank` |\n",
      "| **Prompt engineering (few‚Äëshot, system messages)** | `app/generation.py ‚Üí _SYSTEM_PROMPT` | Add instructions about tone, length, citations |\n",
      "| **Cache frequent queries** | Wrap `answer_query` with `functools.lru_cache` or Redis | Store `query ‚Üí answer` for < 5‚ÄØseconds latency |\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ  TL;DR ‚Äì What You Got\n",
      "\n",
      "* **Full‚Äëstack Python RAG** that can **add / delete** docs on‚Äëthe‚Äëfly.  \n",
      "* **Hybrid retrieval** (vector‚ÄØ+‚ÄØBM25) with a **configurable weight**.  \n",
      "* **LLM‚Äëagnostic streaming generation** (OpenAI, Anthropic, Cohere, Ollama...).  \n",
      "* **FastAPI HTTP SSE endpoint** ready for browsers or any client.  \n",
      "* **Persisted vector DB** (Chroma by default) that survives restarts.  \n",
      "* **Docker‚Äëready** for deployment and a **minimal pytest suite**.\n",
      "\n",
      "You can now drop this repo into any project, point the env‚Äëvars to your preferred models/keys, and start serving dynamic, up‚Äëto‚Äëdate knowledge‚Äëenhanced answers. Happy coding! üöÄ\n"
     ]
    }
   ],
   "source": [
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
