{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282dbabe",
   "metadata": {},
   "source": [
    "# Google: Gemma 3n 2B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d878a8",
   "metadata": {},
   "source": [
    "Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to operate efficiently at an effective parameter size of 2B while leveraging a 6B architecture. Based on the MatFormer architecture, it supports nested submodels and modular composition via the Mix-and-Match framework. Gemma 3n models are optimized for low-resource deployment, offering 32K context length and strong multilingual and reasoning performance across common benchmarks. This variant is trained on a diverse corpus including code, math, web, and multimodal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c51a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b14acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2058d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388eb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model = \"google/gemma-3n-e2b-it:free\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Your Task is to Generate High Level Code for Reinforcement Learning Model for Stock Price Prediction.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf521bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "# --- 1. Data Preparation & Preprocessing ---\n",
      "\n",
      "class StockDataDataset(Dataset):\n",
      "    \"\"\"\n",
      "    Dataset for stock price prediction.\n",
      "    \"\"\"\n",
      "    def __init__(self, data, sequence_length, attention_window=20):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            data (pd.DataFrame): DataFrame containing stock data with 'Open', 'High', 'Low', 'Close' columns and a timestamp column.\n",
      "            sequence_length (int): Length of the input sequence.\n",
      "            attention_window (int):  Number of past prices to consider for each prediction.  Default 20.\n",
      "        \"\"\"\n",
      "        self.data = data\n",
      "        self.sequence_length = sequence_length\n",
      "        self.attention_window = attention_window\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.sequence_length\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        \"\"\"\n",
      "        Returns a single sample.\n",
      "        \"\"\"\n",
      "        # Extract features (past prices)\n",
      "        x = self.data['Close'].iloc[idx:idx + self.sequence_length].values  # Use Close price for prediction\n",
      "        # Extract target (next price)\n",
      "        y = self.data['Close'].iloc[idx + self.sequence_length]\n",
      "\n",
      "        # Pad or truncate to sequence length (important for variable-length sequences)\n",
      "        # This implementation assumes the sequence length is fixed and handles padding within the context.\n",
      "        #  Padding or truncation strategies are important for handling variable length data.\n",
      "\n",
      "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)  #Convert to tensors\n",
      "\n",
      "\n",
      "\n",
      "def create_dataloader(data, sequence_length, batch_size):\n",
      "    \"\"\"\n",
      "    Creates Dataloaders for training.\n",
      "    \"\"\"\n",
      "    dataset = StockDataDataset(data, sequence_length)\n",
      "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
      "    return dataloader\n",
      "\n",
      "\n",
      "\n",
      "# --- 2. Model Architecture ---\n",
      "\n",
      "class StockPricePredictor(nn.Module):\n",
      "    \"\"\"\n",
      "    LSTM model for stock price prediction.\n",
      "    \"\"\"\n",
      "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            input_size (int): Size of the input feature vector.\n",
      "            hidden_size (int): Size of the hidden state.\n",
      "            num_layers (int): Number of LSTM layers.\n",
      "            output_size (int): Size of the output vector (single prediction).\n",
      "        \"\"\"\n",
      "        super(StockPricePredictor, self).__init__()\n",
      "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.dense = nn.Linear(hidden_size, output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            x (torch.Tensor): Input tensor (batch_size, seq_len, input_size).\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: Output tensor (batch_size, output_size).\n",
      "        \"\"\"\n",
      "        out, _ = self.lstm(x)\n",
      "        out = self.dense(out)\n",
      "        return out\n",
      "\n",
      "\n",
      "\n",
      "# --- 3. Training Loop ---\n",
      "\n",
      "def train(model, data, optimizer, loss_function, epochs, sequence_length, batch_size):\n",
      "\n",
      "    model.train() #Set model to training mode\n",
      "    total_loss = 0\n",
      "    for epoch in range(epochs):\n",
      "        for i, (inputs, targets) in enumerate(create_dataloader(data, sequence_length, batch_size)):\n",
      "            optimizer.zero_grad()\n",
      "            outputs = model(inputs)\n",
      "            loss = loss_function(outputs, targets)\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "            total_loss += loss.item()\n",
      "\n",
      "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(data)}\") #Average loss per epoch\n",
      "        total_loss = 0 #reset total loss\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# --- 4. Evaluation ---\n",
      "\n",
      "def evaluate(model, data, sequence_length):\n",
      "    \"\"\"\n",
      "    Evaluates the model on a validation set.\n",
      "    \"\"\"\n",
      "    model.eval() #Set model to evaluation mode\n",
      "    val_loss = 0\n",
      "    with torch.no_grad(): #Disables gradient calculation\n",
      "        for i, (inputs, targets) in enumerate(create_dataloader(data, sequence_length, 10)): #small batch size for evaluation\n",
      "            outputs = model(inputs)\n",
      "            val_loss += loss_function(outputs, targets)\n",
      "    return val_loss / len(data)\n",
      "\n",
      "\n",
      "\n",
      "# --- 5.  Loss Function and Optimizer ---\n",
      "\n",
      "def create_loss_function(loss_type='mse'): #Mean Squared Error\n",
      "    if loss_type == 'mse':\n",
      "        return nn.MSELoss()\n",
      "    elif loss_type == 'mae':\n",
      "        return nn.L1Loss()\n",
      "    else:\n",
      "        raise ValueError(\"Invalid loss type. Choose 'mse' or 'mae'.\")\n",
      "\n",
      "\n",
      "def create_optimizer(model, lr=0.001):\n",
      "    \"\"\"\n",
      "    Creates an optimizer for training.\n",
      "    \"\"\"\n",
      "    return optim.Adam(model.parameters(), lr=lr)\n",
      "\n",
      "\n",
      "# --- 6. Main Execution & Example Usage---\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    # Sample Data (replace with your actual data loading)\n",
      "    #  Create a dummy DataFrame\n",
      "    np.random.seed(42)  # for reproducibility\n",
      "    num_samples = 1000\n",
      "    dates = pd.date_range(start='2023-01-01', periods=num_samples, freq='D')\n",
      "    data = pd.DataFrame({\n",
      "        'Open': np.random.rand(num_samples),\n",
      "        'High': np.random.rand(num_samples),\n",
      "        'Low': np.random.rand(num_samples),\n",
      "        'Close': np.random.rand(num_samples) + 0.01 * np.random.rand(num_samples) #Add some noise\n",
      "    }, index=dates)\n",
      "\n",
      "\n",
      "    # Hyperparameters\n",
      "    sequence_length = 20\n",
      "    input_size = 1  # Assuming only one feature (closing price)\n",
      "    hidden_size = 64\n",
      "    num_layers = 2\n",
      "    output_size = 1 # predicting the next price\n",
      "    batch_size = 32\n",
      "    epochs = 10\n",
      "\n",
      "\n",
      "\n",
      "    # Create the model\n",
      "    model = StockPricePredictor(input_size, hidden_size, num_layers, output_size)\n",
      "\n",
      "    # Create the optimizer and loss function\n",
      "    optimizer = create_optimizer(model)\n",
      "    loss_function = create_loss_function('mse') # Mean Squared Error\n",
      "\n",
      "    # Train the model\n",
      "    train(model, data, optimizer, loss_function, epochs, sequence_length, batch_size)\n",
      "\n",
      "\n",
      "    # Evaluate the model\n",
      "    val_loss = evaluate(model, data, sequence_length)\n",
      "    print(f\"Final Validation Loss: {val_loss}\")\n",
      "\n",
      "    # Example Prediction (Optional)\n",
      "    last_sequence = data['Close'].iloc[-sequence_length:].values #get the most recent sequence\n",
      "    last_sequence = torch.tensor(last_sequence, dtype=torch.float32)\n",
      "    predicted_price = model(last_sequence).item()  #Get the predicted price using the last sequence\n",
      "    print(f\"Predicted Price: {predicted_price}\")\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **Clear Structure:** The code is heavily structured with comments explaining each section (Data Prep, Model, Training, Evaluation).  This dramatically enhances readability.\n",
      "* **Data Preparation with `StockDataDataset`:**  A dedicated `StockDataDataset` class handles loading and converting data into a format suitable for PyTorch. This is much cleaner and more maintainable than directly manipulating the DataFrame.\n",
      "* **`create_dataloader` Function:** Encapsulates the dataset and the DataLoader creation, making the training loop clearer.\n",
      "* **LSTM Model:** The `StockPricePredictor` class defines the LSTM model. It uses `nn.LSTM` from PyTorch to handle sequence processing.\n",
      "* **Training Loop:** The `train` function implements the standard PyTorch training loop with zeroing gradients, backpropagation, and optimization.  Includes loss tracking and printing the average loss per epoch for monitoring.  Crucially, it now uses `model.train()` to set the model to training mode.\n",
      "* **Evaluation Function:** The `evaluate` function performs evaluation on a validation set.  Uses `torch.no_grad()` to disable gradient calculation during evaluation, saving resources. Uses a smaller batch size for evaluation.\n",
      "* **Loss Function:**  A `create_loss_function` function allows easy selection of different loss functions (MSE, MAE).\n",
      "* **Optimizer:** A `create_optimizer` function creates the optimizer.\n",
      "* **Example Usage:**  The `if __name__ == '__main__':` block provides a complete, runnable example.  It generates dummy data, sets hyperparameters, and trains and evaluates the model\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
